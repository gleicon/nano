# Pitfalls Research: v1.2 Production Polish

**Project:** NANO - Zig + V8 JavaScript Runtime
**Milestone:** v1.2 Production Polish
**Features:** Streams API, Per-app Environment Variables, Graceful Shutdown
**Researched:** 2026-02-02
**Confidence:** MEDIUM-HIGH (based on official docs, runtime implementations, security research)

---

## Streams Pitfalls

### S1: Unbounded Internal Queue Growth

- **Risk:** ReadableStream's internal queue grows without bound when producer is faster than consumer. In NANO's arena-per-request model, this could exhaust the request arena before the stream completes, causing OOM or request failure.
- **Warning Signs:**
  - Memory usage spikes during streaming responses
  - Long-running streams consume disproportionate memory
  - Arena allocator reports high watermarks on streaming requests
- **Prevention:**
  - Implement pull-based streaming (use `pull()` not `start()` pumping) as [Deno discovered](https://github.com/denoland/deno/issues/3256)
  - Set explicit highWaterMark values (default 1 chunk for bytes, configurable)
  - Check `controller.desiredSize` before enqueuing; if <= 0, wait for drain
  - Consider separate allocator for stream buffers outside request arena
- **Phase:** Streams API implementation (01-streams)

### S2: TransformStream Backpressure Bypass

- **Risk:** Calling `controller.enqueue()` in TransformStream's `transform()` method ignores backpressure - chunks are enqueued regardless of readable side queue state. This violates the [WHATWG Streams backpressure model](https://github.com/whatwg/streams/issues/1323).
- **Warning Signs:**
  - Transform pipelines consume memory proportional to input size
  - `desiredSize` becomes increasingly negative during transforms
  - Transforms that expand data (compression, encoding) cause OOM
- **Prevention:**
  - Check `controller.desiredSize` in transform before enqueuing
  - If `desiredSize <= 0`, return a Promise that resolves when queue drains
  - Document this behavior for app developers (differs from intuition)
  - Consider implementing `TransformStream` with explicit backpressure hooks
- **Phase:** Streams API implementation (01-streams)

### S3: Chunk Lifetime Across V8/Zig Boundary

- **Risk:** Stream chunks cross the V8/Zig boundary multiple times. If chunk data is backed by Zig memory (arena), that memory may be freed while V8 still holds a reference. Conversely, V8-allocated chunks may leak if not properly released.
- **Warning Signs:**
  - Use-after-free crashes during streaming
  - Memory growth on repeated stream operations
  - Corrupted data in stream output
- **Prevention:**
  - Clear ownership model: chunks either owned by V8 (ArrayBuffer) or Zig (copied on boundary cross)
  - Use V8's `ArrayBuffer::Externalize()` for Zig-owned buffers with custom destructor
  - Never hold raw Zig pointers in V8 objects across async boundaries
  - Test with AddressSanitizer enabled
- **Phase:** Streams API implementation (01-streams)

### S4: BYOB Reader Memory Management

- **Risk:** Bring Your Own Buffer (BYOB) readers allow apps to provide their own ArrayBuffer for reading. If the buffer is detached, resized, or transferred during read, undefined behavior occurs.
- **Warning Signs:**
  - Crashes when using BYOB readers
  - Data corruption with custom buffers
  - ArrayBuffer detachment errors
- **Prevention:**
  - Validate buffer is not detached before read operations
  - Copy data if buffer ownership is uncertain
  - Initially implement default readers only; BYOB as later optimization
  - Document BYOB limitations clearly
- **Phase:** Streams API implementation - defer BYOB to later if complex

### S5: Async Iterator Protocol Mismatch

- **Risk:** ReadableStream async iteration (`for await...of`) creates a reader that must be released. If iteration breaks early (break, throw, return), the reader may not release, locking the stream permanently.
- **Warning Signs:**
  - Streams become "locked" after partial reads
  - Subsequent read attempts fail with "reader already acquired"
  - Memory leaks from unreleased readers
- **Prevention:**
  - Implement `return()` method on async iterator to release reader
  - Wrap iteration in try/finally in documentation examples
  - Consider auto-release on iterator GC (weak reference pattern)
- **Phase:** Streams API implementation (01-streams)

---

## Graceful Shutdown Pitfalls

### G1: Signal Handler Race with Event Loop

- **Risk:** NANO's signal handler sets `running = false` and makes a dummy connection to unblock accept(). But if a signal arrives while processing the event loop or V8 execution, state may be inconsistent. [Signal handler race conditions](https://cwe.mitre.org/data/definitions/364.html) are a known vulnerability class (CWE-364).
- **Warning Signs:**
  - Crashes during shutdown under load
  - Incomplete cleanup (V8 isolates not destroyed)
  - Hung shutdown (never exits)
- **Prevention:**
  - Signal handler should ONLY set an atomic flag, nothing else
  - Use `signalfd()` or self-pipe trick to integrate signals with event loop
  - Check shutdown flag at well-defined points (after accept, after request complete)
  - Never call complex functions (malloc, logging) from signal handler
- **Phase:** Graceful shutdown (02-shutdown)

### G2: In-Flight Request Tracking Missing

- **Risk:** Current implementation has no tracking of in-flight requests. On shutdown, `running = false` stops the accept loop, but requests currently being processed have no grace period. The server may close the socket while V8 is still executing, corrupting response.
- **Warning Signs:**
  - Clients receive partial responses during shutdown
  - Requests in progress return 5xx or connection reset
  - No "draining" log messages during shutdown
- **Prevention:**
  - Add atomic counter: increment on request start, decrement on complete
  - Shutdown sequence: (1) stop accepting, (2) wait for counter == 0 or timeout, (3) force close
  - Set maximum drain timeout (e.g., 30 seconds) to prevent hung shutdown
  - Log in-flight count during shutdown for observability
- **Phase:** Connection draining (02-shutdown)

### G3: Keep-Alive Connection Drain Race

- **Risk:** HTTP/1.1 keep-alive connections may have new requests arrive after shutdown starts but before the connection closes. [This causes TCP RST](https://github.com/nodejs/node/issues/60617) and client errors.
- **Warning Signs:**
  - Clients see "connection reset by peer" during rolling deploys
  - Increased error rates during shutdown window
  - Works fine with single requests, fails with keep-alive
- **Prevention:**
  - NANO currently sets `Connection: close` on every response (no keep-alive) - verify this
  - If implementing keep-alive later: drain period where idle connections get `Connection: close` on next request
  - Consider HTTP/2 GOAWAY frame semantics if adding HTTP/2
- **Phase:** Connection draining (02-shutdown)

### G4: V8 Isolate Termination During Shutdown

- **Risk:** When shutting down, V8 isolates running user code must be terminated. `isolate->TerminateExecution()` is async - the isolate may continue executing for a brief period. Disposing isolate while code runs causes crash.
- **Warning Signs:**
  - Crashes in V8 code during shutdown
  - "Isolate is already disposed" errors
  - Segfaults in HandleScope operations
- **Prevention:**
  - Call `TerminateExecution()` on all isolates first
  - Wait for all request handlers to complete (they check termination flag)
  - Only then call `isolate->Dispose()`
  - Set hard timeout: if isolates don't terminate within X seconds, log and force exit
- **Phase:** Graceful shutdown (02-shutdown)

### G5: Config Watcher Shutdown Race

- **Risk:** The ConfigWatcher polls on a timer in the event loop. If shutdown occurs mid-reload, partially loaded apps or half-updated routing tables may cause crashes or routing errors.
- **Warning Signs:**
  - Crashes during config reload + shutdown combination
  - Apps partially loaded on restart
  - Routing table in inconsistent state
- **Prevention:**
  - Stop ConfigWatcher FIRST in shutdown sequence
  - Add mutex/lock around config reload operations
  - Make config reload atomic: build new state fully, then swap
  - Test: send SIGTERM during config file write
- **Phase:** Graceful shutdown (02-shutdown)

### G6: Event Loop Timer Cleanup

- **Risk:** libxev timers scheduled by app code (setTimeout/setInterval) must be cancelled on shutdown. If not, timer callbacks may fire after isolate disposal.
- **Warning Signs:**
  - Callbacks execute after app cleanup
  - Use-after-free in timer callback code
  - Event loop never exits (active timers keep it alive)
- **Prevention:**
  - Track all active timers per app
  - On app removal: cancel all associated timers
  - On shutdown: cancel all timers before disposing isolates
  - `EventLoop.hasPendingWork()` should return false before full shutdown
- **Phase:** Graceful shutdown (02-shutdown)

---

## Connection Draining Pitfalls

### D1: Request Timeout vs Drain Timeout Conflict

- **Risk:** NANO has per-request timeouts (watchdog, 5s default). During drain, long-running requests should complete, but watchdog may terminate them. Conversely, requests without watchdog (if any) may block drain forever.
- **Warning Signs:**
  - Legitimate requests timeout during drain period
  - Drain period exceeds expected time
  - Some requests complete, others terminated inconsistently
- **Prevention:**
  - Drain timeout is separate from request timeout
  - During drain: request timeout still applies (prevents hung requests)
  - Drain timeout > typical request timeout (e.g., drain=30s, request=5s)
  - Log which requests are blocking drain
- **Phase:** Connection draining (02-shutdown)

### D2: Memory Pressure During Drain

- **Risk:** During drain, multiple in-flight requests continue processing. Memory pressure may spike if many concurrent requests. OOM during drain could cause unclean shutdown.
- **Warning Signs:**
  - Memory spikes during shutdown
  - OOM kills during rolling restart
  - Arena allocator exhaustion during drain
- **Prevention:**
  - Stop accepting new requests immediately (reduce new memory allocation)
  - Consider request queue limit even during drain
  - If memory critical: force-terminate oldest requests first
  - Monitor memory during drain period
- **Phase:** Connection draining (02-shutdown)

### D3: App Removal Without Request Drain

- **Risk:** Admin API `DELETE /admin/apps?hostname=X` removes app immediately. In-flight requests for that hostname lose their isolate reference, causing crash or undefined behavior.
- **Warning Signs:**
  - Crashes on DELETE during active traffic
  - Requests return 500 after app removal
  - Null pointer access in handleRequest
- **Prevention:**
  - Mark app as "draining" first (stop routing new requests)
  - Wait for in-flight requests to that app to complete
  - Only then dispose isolate and remove from registry
  - Return 503 for new requests to draining apps
- **Phase:** Connection draining (02-shutdown) - extend to per-app draining

### D4: WebSocket Future Compatibility

- **Risk:** WebSocket connections (v2+ feature) are long-lived. Drain logic designed for request/response won't work - WebSocket needs explicit close frame, graceful handshake.
- **Warning Signs:**
  - (Future) WebSocket connections severed without close frame
  - Clients don't receive graceful close
  - Connection state leaked
- **Prevention:**
  - Design drain interface to be extensible (not HTTP-specific)
  - Document that current drain is HTTP-only
  - When adding WebSocket: integrate with drain (GOAWAY-like pattern)
- **Phase:** Design consideration for 02-shutdown, implementation in v2+

---

## Environment Variable Pitfalls

### E1: Process.env Contamination

- **Risk:** If env vars are implemented by modifying `process.env` or global state, one app's env vars could leak to another app. [This is a critical security issue](https://medium.com/@instatunnel/how-your-environment-variables-can-betray-you-in-production-the-hidden-security-risks-developers-d77200b5cda9) in multi-tenant environments.
- **Warning Signs:**
  - App A can read App B's secrets
  - Env vars persist across app reloads
  - `globalThis.process.env` shows combined vars from all apps
- **Prevention:**
  - NEVER modify actual process env or global state
  - Create per-isolate `env` object on context global
  - Env object is frozen (immutable) after creation
  - Each isolate gets its own copy, not a reference
  - Test: App A sets env, App B should not see it
- **Phase:** Environment variables (03-env-vars)

### E2: Env Vars Visible via Side Channels

- **Risk:** Even with isolation, env vars could leak via: error messages including env values, timing attacks (different behavior based on env), logging that includes env values.
- **Warning Signs:**
  - Stack traces contain env var values
  - Error messages reveal secrets
  - Log aggregation exposes env vars
- **Prevention:**
  - Sanitize error messages before returning to client
  - Never log env var values (log keys only if needed)
  - Consider: separate "secrets" (never logged) from "config" (may be logged)
  - Redact env vars in stack traces
- **Phase:** Environment variables (03-env-vars)

### E3: Env Vars Not Updated on App Reload

- **Risk:** Config hot-reload updates app code but may not update env vars if they're set at isolate creation time. Stale secrets could persist.
- **Warning Signs:**
  - Changing env vars in config has no effect
  - Must restart entire server to pick up env changes
  - Security rotation doesn't take effect
- **Prevention:**
  - Env vars are re-read from config on app reload
  - Treat env var change as requiring isolate recreation (not just script reload)
  - Document: env var changes require app reload, not just config file save
- **Phase:** Environment variables (03-env-vars)

### E4: Large Env Var Memory Cost

- **Risk:** Each app gets a copy of its env vars. With many apps and large env var sets, memory usage scales with (apps * env_size). [Cloudflare limits to 128 vars, 5KB each](https://developers.cloudflare.com/workers/platform/limits/).
- **Warning Signs:**
  - Memory usage high with many apps
  - Large env vars cause OOM
  - Env var parsing slow on app load
- **Prevention:**
  - Set limits: max vars (64), max size per var (5KB), max total (64KB)
  - Enforce limits at config parse time with clear errors
  - Consider: intern common env var values (shared immutable strings)
- **Phase:** Environment variables (03-env-vars)

### E5: Prototype Pollution via Env Object

- **Risk:** If env object is a plain JS object, apps could pollute its prototype, affecting other operations. `env.__proto__.SECRET = "hacked"` could leak.
- **Warning Signs:**
  - Prototype chain manipulation affects env
  - `Object.prototype` modifications visible in env
  - `hasOwnProperty` checks bypassed
- **Prevention:**
  - Create env with `Object.create(null)` - no prototype
  - Freeze object after population: `Object.freeze(env)`
  - Use V8 object template with null prototype
  - Test: attempt prototype pollution, verify isolation
- **Phase:** Environment variables (03-env-vars)

### E6: Env Var Encoding Issues

- **Risk:** Env var values may contain non-ASCII characters, special characters, or binary data. UTF-8/UTF-16 conversion at V8 boundary could corrupt values.
- **Warning Signs:**
  - Unicode env var values corrupted
  - Special characters (=, newlines) cause parsing errors
  - Binary values truncated at null bytes
- **Prevention:**
  - Validate env var names/values at parse time
  - Use consistent UTF-8 encoding throughout
  - Escape or reject problematic characters in names (=, null)
  - Document supported character set
- **Phase:** Environment variables (03-env-vars)

---

## Integration Pitfalls (Combining with Existing NANO)

### I1: Arena Allocator + Streaming Conflict

- **Risk:** NANO uses arena allocator per request for instant cleanup. Streams may outlive the request if data is returned before stream completes. Arena freed while stream still buffering = use-after-free.
- **Warning Signs:**
  - Crashes with streaming responses
  - Corrupted stream data
  - Works with small responses, fails with large streams
- **Prevention:**
  - Streams need separate allocation strategy (not request arena)
  - Options: (a) stream owns its buffer, (b) reference counting, (c) copy to V8 heap
  - Consider: response complete only when stream fully consumed
  - This may require rethinking `handleRequest` return model
- **Phase:** Streams API - critical design decision

### I2: Watchdog Termination + Streams

- **Risk:** CPU watchdog terminates long-running scripts. Streaming response may legitimately run longer than watchdog timeout (5s default). Terminating mid-stream corrupts response.
- **Warning Signs:**
  - Streaming responses timeout unexpectedly
  - Large file downloads fail
  - Watchdog fires during legitimate I/O wait
- **Prevention:**
  - Watchdog should track CPU time, not wall clock time
  - I/O wait (stream read/write) should not count against watchdog
  - Alternative: extend timeout for streaming responses
  - Consider: per-operation timeouts vs global watchdog
- **Phase:** Streams API + existing watchdog integration

### I3: Event Loop Tick + Streaming

- **Risk:** `processEventLoop` currently runs after request handler returns. Streaming requires event loop ticks during response streaming. Current model may not support this.
- **Warning Signs:**
  - Stream chunks not sent until request completes
  - Backpressure signals not propagated
  - Event loop callbacks delayed
- **Prevention:**
  - Review event loop integration for streaming use case
  - May need to tick event loop during stream operations
  - Consider: yield points in stream read/write for event processing
- **Phase:** Streams API - event loop integration

### I4: Multi-App Routing + Env Isolation

- **Risk:** Multiple apps share the same V8 platform and ArrayBufferAllocator. If env vars are stored on shared structures, isolation fails.
- **Warning Signs:**
  - Env vars leak between apps
  - Changing one app's env affects another
  - Shared state modifications visible cross-app
- **Prevention:**
  - Env vars stored per-isolate, not per-platform
  - Verify: each App struct has its own env storage
  - Test with multiple apps having same env key, different values
- **Phase:** Environment variables (03-env-vars)

### I5: Hot Reload + Graceful Shutdown Interaction

- **Risk:** Config reload creates new app, disposes old. This is similar to shutdown but for single app. If drain logic differs, bugs may appear only in reload or only in shutdown.
- **Warning Signs:**
  - Reload works, shutdown fails (or vice versa)
  - In-flight requests handled differently in each case
  - Code duplication with subtle differences
- **Prevention:**
  - Extract common "app drain and dispose" logic
  - Use same code path for: app removal, config reload, full shutdown
  - Test matrix: reload during traffic, shutdown during traffic, both
- **Phase:** Graceful shutdown (02-shutdown)

---

## Phase-Specific Warnings Summary

| Phase | Critical Pitfalls | Must Address |
|-------|------------------|--------------|
| 01-streams | S1 (unbounded queue), S3 (chunk lifetime), I1 (arena conflict) | Memory model for streams |
| 02-shutdown | G1 (signal race), G2 (in-flight tracking), D3 (app removal drain) | Request tracking, safe signal handling |
| 03-env-vars | E1 (process.env contamination), E5 (prototype pollution) | Per-isolate isolation, frozen objects |

---

## Testing Strategies

### Streams Testing
```javascript
// Test: backpressure respected
const slow = new WritableStream({
  write(chunk) { return new Promise(r => setTimeout(r, 100)); }
});
const fast = new ReadableStream({
  pull(c) { c.enqueue(new Uint8Array(1024)); } // Should slow down
});
fast.pipeTo(slow); // Monitor memory - should stay bounded
```

### Shutdown Testing
```bash
# Test: graceful shutdown with in-flight requests
for i in {1..100}; do curl -s "http://localhost:8080/slow" & done
kill -TERM $NANO_PID
# Verify: all requests complete or get proper error, no crashes
```

### Env Isolation Testing
```javascript
// App A
console.log(env.SECRET); // "app-a-secret"
env.__proto__.INJECTED = "hacked"; // Should fail or be isolated

// App B (separate test)
console.log(env.SECRET); // "app-b-secret"
console.log(env.INJECTED); // Should be undefined
```

---

## Sources

### Streams
- [WHATWG Streams Standard](https://streams.spec.whatwg.org/)
- [Deno ReadableStream memory issue](https://github.com/denoland/deno/issues/3256)
- [WHATWG Streams backpressure handling](https://github.com/whatwg/streams/issues/1323)
- [Cloudflare Workers Streams implementation](https://blog.cloudflare.com/standards-compliant-workers-api/)

### Graceful Shutdown
- [Graceful Shutdown in Go](https://rafalroppel.medium.com/graceful-shutdown-in-go-explained-signals-contexts-and-the-correct-shutdown-sequence-f24fd9ef8fac)
- [Node.js HTTP connection draining](https://github.com/nodejs/node/issues/60617)
- [gRPC Graceful Stop](https://grpc.io/docs/guides/server-graceful-stop/)
- [Signal Handler Race Conditions (CWE-364)](https://cwe.mitre.org/data/definitions/364.html)

### Environment Variables
- [Cloudflare Workers Environment Variables](https://developers.cloudflare.com/workers/development-testing/environment-variables/)
- [Multi-tenant env isolation](https://medium.com/@instatunnel/how-your-environment-variables-can-betray-you-in-production-the-hidden-security-risks-developers-d77200b5cda9)
- [Cloudflare Workers Security Model](https://developers.cloudflare.com/workers/reference/security-model/)
- [JavaScript Sandbox Security](https://snyk.io/blog/security-concerns-javascript-sandbox-node-js-vm-module/)

### V8/Memory
- [V8 Memory Leak Testing](https://joyeecheung.github.io/blog/2024/03/17/memory-leak-testing-v8-node-js-1/)
- [Node.js Backpressure in Streams](https://nodejs.org/en/learn/modules/backpressuring-in-streams)
- [V8 Memory Leak Investigation](https://v8.dev/docs/memory-leaks)

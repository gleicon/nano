# v1.2 Production Polish: Research Synthesis

**Milestone:** NANO v1.2 Production Polish
**Researched:** 2026-02-02
**Synthesized:** 2026-02-02

---

## Executive Summary

v1.2 targets three tightly scoped production features: WinterCG Streams API, per-app environment variables, and graceful shutdown with connection draining. All three are well-established patterns in the edge computing ecosystem (Cloudflare, Deno, Vercel), with mature specifications and proven implementations. **The recommended approach prioritizes environment variables first (lowest risk, immediate value), then Streams API (required WinterCG compliance), then graceful shutdown (complex coordination).** Stack additions are minimal—one JavaScript polyfill bundled into runtime, native Zig HashMap for config, and architectural refinements to http.zig server loop. Key risk: Streams integration with NANO's arena-per-request memory model requires careful design to prevent use-after-free bugs.

---

## Key Findings by Research Area

### Stack (v1.2-STACK.md)

**Core Technologies:**
- **Streams:** web-streams-polyfill 4.2.0 (JavaScript, MIT license) over native C++ bindings
  - Rationale: 1-2 day integration vs weeks for native C++; sufficient performance for v1.2
- **Config Extensions:** Native Zig HashMap for env vars (no external deps)
- **Graceful Shutdown:** Custom implementation in http.zig (Zig std.net.Server compatible)
- **Docs Framework:** Astro 5.x + Starlight 0.37 (deployment: Vercel)

**Integration Points:**
- Streams polyfill bundled into V8 context at app bootstrap (not npm dependency)
- Config JSON gains `"env"` field per app
- http.zig gets connection tracking + drain timeout + draining state
- Admin API unchanged for v1.2

**Deferred to v1.3:**
- CompressionStream/DecompressionStream (WinterCG optional)
- Request.body as ReadableStream (currently body is string)
- Native Streams fast paths (polyfill sufficient for v1.2)

**Confidence:** HIGH on stack selection, MEDIUM-HIGH on implementation timeline

---

### Features (FEATURES.md)

**Table Stakes (Must Implement):**
1. **ReadableStream, WritableStream, TransformStream** — WinterTC Minimum Common API mandates these
2. **Graceful shutdown for SIGTERM/SIGINT** — Standard expectation across platforms
3. **Per-app env vars via config** — Universal across Workers-compatible platforms

**Differentiators (Should Implement):**
1. **ReadableStream.from()** — Create stream from async iterable (low complexity)
2. **Per-app drain visibility** — Admin API shows which apps draining (ops visibility)
3. **Runtime env var updates** — Admin API can modify env without reload (edge platforms can't)

**Anti-Features (Explicitly Avoid):**
1. **Unrestricted stream lifetime** — Streams must tie to request context (prevent memory leaks)
2. **Keep-alive connection leaks** — Track and close all connections during drain
3. **process.env global state** — Each app gets isolated env object, not global process.env
4. **Env var inheritance from host** — Apps see only configured vars, never system env

**Risk Flags:**
- Streams inside request context only (Cloudflare pattern) — requires careful lifecycle design
- Backpressure handling is critical (Deno learned this hard way)
- Env isolation must be airtight (security-critical for multi-tenant)

**Confidence:** HIGH on features, MEDIUM on implementation complexity estimates

---

### Architecture (ARCHITECTURE.md)

**Integration Dependencies:**
```
Per-App Env Vars (INDEPENDENT)
  └─ config.zig parsing
  └─ app.zig injected at isolate creation
  └─ handleRequest passes to fetch handler

Streams API (INDEPENDENT, but must precede Response refactoring)
  └─ src/api/streams.zig (new file)
  └─ fetch.zig Response.body integration (future phase)
  └─ Memory model conflicts with arena allocator (design risk)

Graceful Shutdown (DEPENDS on connection tracking)
  └─ HttpServer connection tracking (new)
  └─ App lifecycle state enum (new)
  └─ Drain timeout management (new)
  └─ Integration with ConfigWatcher (modify)
```

**Suggested Build Order (from architecture analysis):**
1. **Phase 01: Per-App Env Vars** (self-contained, quick)
2. **Phase 02: Streams API Foundation** (ReadableStream + Reader, foundation for later)
3. **Phase 03: Response.body Integration** (depends on phase 02)
4. **Phase 04: Connection Tracking** (foundation for draining)
5. **Phase 05: Graceful Shutdown** (app removal + process shutdown)

**Key Architectural Patterns to Preserve:**
- Arena allocator per request (but streams need separate buffer strategy)
- V8 isolate per app with persistent handles
- Function pointer callbacks (avoid circular imports)
- libxev event loop (poll-based ConfigWatcher)

**Confidence:** HIGH on architecture review, MEDIUM on memory model resolution for streams

---

### Pitfalls (v1.2-PITFALLS.md)

**Critical (Phase-Blocking):**

| Phase | Pitfall | Prevention |
|-------|---------|-----------|
| **Streams** | S1: Unbounded queue growth | Use pull-based streaming, enforce highWaterMark, check desiredSize |
| **Streams** | S3: Chunk lifetime across boundary | Clear ownership: ArrayBuffer for V8, copy or Externalize for Zig |
| **Streams** | I1: Arena conflict | Streams need separate allocator, not request arena |
| **Shutdown** | G1: Signal handler race | Signal handler only sets atomic flag; use signalfd/self-pipe |
| **Shutdown** | G2: No in-flight tracking | Add atomic counter; wait for 0 or timeout |
| **Env Vars** | E1: process.env contamination | Per-isolate env object, never modify global state |

**High (Testing Required):**

| Phase | Pitfall | Prevention |
|-------|---------|-----------|
| **Streams** | S2: TransformStream backpressure | Check desiredSize, return Promise if <= 0 |
| **Streams** | S5: Async iterator leak | Implement return() for early exit |
| **Shutdown** | G4: V8 isolation termination | TerminateExecution first, wait, then Dispose |
| **Shutdown** | G5: ConfigWatcher race | Stop watcher first, make reload atomic |
| **Env Vars** | E5: Prototype pollution | Object.create(null), Object.freeze() |

**Integration Risks:**

| Risk | Impact | Mitigation |
|------|--------|-----------|
| **I2: Watchdog + Streams** | Long streams timeout prematurely | Watchdog tracks CPU time, not wall clock |
| **I3: Event loop + Streams** | Stream chunks delayed until request complete | May need event loop ticks during streaming |
| **I5: Reload vs Shutdown** | Different code paths, different behaviors | Extract common drain logic, test both |

**Confidence:** MEDIUM-HIGH (based on official docs and runtime implementations); requires validation during implementation

---

## Implications for Roadmap

### Phase Structure (Recommended Order)

#### Phase 01: Per-App Environment Variables (Week 1)
**Rationale:** Lowest complexity, no dependencies, immediate value.

**Features:**
- Config.json `"env"` field per app
- V8 isolate injection at request time
- Cloudflare Workers API: `fetch(request, env)`

**Pitfalls to Avoid:**
- E1 (contamination) — strict per-isolate isolation
- E5 (pollution) — Object.create(null) + freeze
- E3 (reload stale) — env re-read on app reload

**Deliverable:** Apps can use environment-specific secrets without code changes

---

#### Phase 02: Streams API Foundation (Weeks 2-3)
**Rationale:** Required for WinterCG compliance; foundation for Response.body integration.

**Features:**
- ReadableStream + ReadableStreamDefaultReader
- WritableStream + WritableStreamDefaultWriter
- TransformStream core
- Backpressure handling (desiredSize checks)

**Do NOT Include (defer to v1.3):**
- BYOB readers (complexity, can optimize later)
- CompressionStream/DecompressionStream (optional for v1.2)
- Request.body streaming (Response refactor comes in phase 3)

**Pitfalls to Avoid:**
- S1 (queue growth) — pull-based design, highWaterMark limits
- S3 (chunk lifetime) — clear V8/Zig ownership model
- I1 (arena conflict) — separate allocator for stream buffers
- I3 (event loop) — design for tick during streaming

**Deliverable:** Standalone Streams API available to user code; benchmarks show backpressure works

---

#### Phase 03: Response.body Integration (Week 4)
**Rationale:** Completes streaming story; depends on phase 02.

**Features:**
- Response.body getter returns ReadableStream
- Response.text()/json()/arrayBuffer() handle streaming
- fetch() option for streaming response bodies

**Pitfalls to Avoid:**
- I2 (watchdog) — verify no timeout during legitimate streaming
- Backwards compatibility — Response constructor still accepts string bodies

**Deliverable:** Streaming responses work end-to-end; large file downloads possible

---

#### Phase 04: Connection Tracking (Week 4 concurrent)
**Rationale:** Foundation for graceful shutdown; low risk, additive.

**Features:**
- HttpServer tracks active connections
- Per-app pending request count
- Metrics exposed (optional)

**Pitfalls to Avoid:**
- Race conditions during concurrent request starts/stops
- Memory overhead of tracking HashMap

**Deliverable:** Server can report active request counts; foundation for draining

---

#### Phase 05: Graceful Shutdown (Weeks 5-6)
**Rationale:** Most complex; depends on phase 04.

**Sub-Phase 5a: App Removal Draining**
- Mark app as "draining" on removal
- Return 503 for new requests
- Wait for in-flight to complete or timeout
- Then dispose isolate

**Sub-Phase 5b: Process Shutdown Draining**
- Signal handler safety (atomic flag only)
- Stop accepting new connections
- Wait for all in-flight requests
- Drain timeout configurable (default 30s)
- Force close remaining connections on timeout

**Pitfalls to Avoid:**
- G1 (signal race) — signal handler discipline
- G2 (no tracking) — use phase 04 tracking
- G4 (isolate termination) — TerminateExecution before Dispose
- G5 (ConfigWatcher race) — stop watcher first
- D1 (timeout conflict) — drain timeout independent of request timeout

**Deliverable:** SIGTERM triggers graceful shutdown; in-flight requests complete; rolling deploy pattern works

---

### Research Flags

**Phases Needing Deeper Research:**
- **Phase 02 (Streams):** Memory model resolution needed before coding
  - How to handle chunks crossing V8/Zig boundary safely?
  - Can request arena support streaming, or need separate allocator?
  - Recommend: spike on allocation strategy before phase start

- **Phase 05 (Graceful Shutdown):** Signal handling integration unclear
  - Current std.net.Server interaction with SIGTERM needs validation
  - Recommend: test signal handler race conditions before full implementation

**Phases with Standard Patterns (Can Proceed):**
- **Phase 01 (Env Vars):** Cloudflare pattern is well-documented, no unknowns
- **Phase 03 (Response.body):** WHATWG spec is clear, integration straightforward
- **Phase 04 (Tracking):** Standard connection tracking pattern, no surprises

---

## Confidence Assessment

| Area | Level | Notes |
|------|-------|-------|
| **Stack Choices** | HIGH | Polyfill approach validated by Cloudflare, Deno, workerd |
| **Feature Requirements** | HIGH | WinterTC spec defines table stakes clearly |
| **Architecture Integration** | MEDIUM-HIGH | Existing codebase well-structured; memory model for streams needs validation |
| **Implementation Timeline** | MEDIUM | Estimates based on spec complexity; actual depends on V8 binding complexity |
| **Graceful Shutdown Patterns** | HIGH | Industry patterns proven in Node.js, Go, gRPC |
| **Env Isolation Security** | HIGH | Cloudflare model proven; implementation straightforward with care |

---

## Gaps to Address During Planning

### Before Phase 02 Starts
1. **Streams Memory Model Decision** — Architecture phase found conflict between streams and arena allocator
   - Decision needed: separate allocator? reference counting? copy to V8 heap?
   - Spike: test prototype with web-streams-polyfill in similar architecture

2. **Watchdog Timeout Interaction** — Current 5s CPU timeout may conflict with streaming
   - Decision needed: change watchdog to track CPU only? extend for streams?
   - Validation: benchmark large file streaming with current watchdog

### Before Phase 05 Starts
3. **Signal Handler Safety Validation** — Current dummy connection approach works but fragile
   - Testing needed: verify signal handler race conditions under load
   - Alternative: switch to signalfd integration with event loop

4. **ConfigWatcher Reload Atomicity** — Hot reload could race with shutdown
   - Testing needed: verify config reload doesn't corrupt state during drain
   - Refactor: extract atomic swap pattern, reuse for shutdown

### Validation Gates
- **After Phase 02:** Load test with 10MB+ streaming responses; verify memory stays bounded
- **After Phase 04:** Concurrent connection tracking under 1000 req/sec; measure overhead
- **After Phase 05:** Kill -9 during active requests, verify graceful shutdown completes in <30s

---

## Sources Aggregated

**Specifications:**
- [WinterTC Minimum Common API](https://min-common-api.proposal.wintertc.org/)
- [WHATWG Streams Standard](https://streams.spec.whatwg.org/)

**Runtime Implementations:**
- [Cloudflare Workers](https://developers.cloudflare.com/workers/)
- [Deno Deploy](https://docs.deno.com/deploy/)
- [web-streams-polyfill](https://github.com/MattiasBuelens/web-streams-polyfill) (4.2.0 recommended)

**Production Patterns:**
- Node.js graceful shutdown (blog.risingstack.com)
- PM2 best practices (pm2.io)
- gRPC graceful stop (grpc.io)
- Signal handler safety (CWE-364)

**NANO Codebase:**
- v1.1 source code architecture review (src/config.zig, src/server/http.zig, src/api/*)

---

## Roadmap Summary for Orchestrator

**v1.2 consists of 5 sequential phases across 6 weeks:**

1. **Phase 01** (Week 1): Per-App Env Vars — table stakes, immediate value
2. **Phase 02** (Weeks 2-3): Streams API Foundation — WinterCG compliance, requires memory model design
3. **Phase 03** (Week 4): Response.body Integration — completes streaming story
4. **Phase 04** (Week 4 concurrent): Connection Tracking — foundation for draining
5. **Phase 05** (Weeks 5-6): Graceful Shutdown — most complex, two sub-phases

**Key Risks:**
- Streams + arena allocator memory model (design risk, spike recommended)
- Signal handler race conditions (validation risk, test under load)
- Event loop integration with streaming (implementation risk, moderate)

**Quick Wins:**
- Phase 01 (env vars) can ship independently
- Env + graceful shutdown enable production ops improvements immediately

**Ready for Requirements:** All phases have clear user stories, acceptance criteria derivable from research. Architecture research identified necessary design decisions; proceed to detailed feature specs with gaps addressed.

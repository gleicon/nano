# Phase v1.2-04: Graceful Shutdown - Research

**Researched:** 2026-02-07
**Domain:** Signal handling, connection lifecycle management, request draining, app removal
**Confidence:** HIGH

## Summary

Phase v1.2-04 implements graceful shutdown for the NANO server—both process-wide (SIGTERM/SIGINT) and per-app (removal via config watcher). The phase requires six coordinated features: stopping new connection acceptance, draining in-flight requests with a timeout, returning 503 Service Unavailable to new requests targeting apps being removed, tracking active connections per app, and allowing the process to exit cleanly even if some connections hang.

The technical challenge is moderate: NANO already has signal handlers installed (lines 912-920 in http.zig) and a config watcher for hot-reload (event_loop.zig, ConfigWatcher). The missing pieces are (1) per-app connection tracking, (2) a "draining" state for both the server and individual apps, (3) logic to return 503 for requests targeting draining apps, and (4) a coordinated drain-and-exit sequence with a configurable timeout (30s default).

The architecture is straightforward: add connection tracking to HttpServer (map of app hostname → active connection count), add a `draining` flag to HttpServer and per-app state, modify request routing to check draining state and return 503, and implement a drain loop that waits for connections to close before exit.

**Primary recommendation:** Track connections per app using an `AppDrainState` struct containing active connection count and drain start time. On SIGTERM/SIGINT, set server draining state and stop accept(). On config removal, mark app as draining and return 503 to new requests. Drain-wait loop polls for zero active connections or timeout before exit. Implement as 3 focused sub-tasks: (1) connection tracking, (2) draining state machine, (3) drain-wait sequence.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| std.posix.Sigaction | Zig stdlib | Install SIGTERM/SIGINT handlers | Already used in http.zig lines 912-920 |
| std.time.nanoTimestamp() | Zig stdlib | Track drain start time and measure timeout | Already used for request latency in http.zig |
| xev.Loop tick() | libxev (existing) | Poll event loop during drain wait | Already used in event_loop.zig for timers |
| std.StringHashMap | Zig stdlib | Map app hostname → drain state | Standard pattern in http.zig for app storage |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| log.stdout() / log.stderr() | NANO log module (existing) | Log graceful shutdown events | All shutdown state transitions should log |
| metrics | metrics_mod (existing) | Record uptime and request counts during shutdown | Already used in http.zig.stop() |

### Integration Points
- **Signal handlers:** Already exist (http.zig lines 912-920), will call `server.stop()` with new draining logic
- **Config watcher:** Calls `removeApp()` on app removal (http.zig lines 245-280), will check drain state
- **Request handling:** `handleConnection()` must check app draining state (http.zig lines 391-542)
- **Server run loop:** Main accept/handle loop must check `server.running` and draining state (http.zig lines 353-389)

## Architecture Patterns

### Graceful Shutdown State Machine

```
NORMAL
  ├─ SIGTERM received → DRAINING (stop accepting, drain active)
  ├─ App removed → APP_DRAINING (just that app drains)
  └─ No change → continue accepting

DRAINING
  ├─ All connections close → EXIT
  ├─ Timeout (30s) expires → FORCE_EXIT
  └─ No change → continue draining
```

**Implementation:** Add state tracking to HttpServer:
- `draining: bool` (server is shutting down, don't accept new connections)
- Per-app: `AppDrainState` struct with `draining: bool`, `active_connections: u64`, `drain_start_ns: i128`

### Pattern 1: Per-App Connection Tracking

**What:** Track active connections for each app to know when drain is complete.

**When to use:** Every app that can be removed via config watcher (multi-app mode).

**Implementation:**

```zig
const AppDrainState = struct {
    hostname: []const u8,
    active_connections: u64 = 0,
    draining: bool = false,
    drain_start_ns: i128 = 0,
};

// In HttpServer
app_drain_state: std.StringHashMap(AppDrainState),

// On request start, increment for target app
if (target_app) |a| {
    if (self.app_drain_state.getPtr(hostname)) |drain| {
        drain.active_connections += 1;
    }
}

// On request end, decrement
if (target_app) |_| {
    if (self.app_drain_state.getPtr(hostname)) |drain| {
        drain.active_connections -|= 1; // Saturating subtract
    }
}
```

**Key insight:** Track by hostname (the map key) rather than App pointer, since app objects can be deallocated during config reload. Hostname is stable and unique.

### Pattern 2: Return 503 for Draining Apps

**What:** Route requests targeting a draining app to 503 Service Unavailable.

**When to use:** After app is marked for removal via config watcher or during server drain.

**Implementation:**

```zig
// In handleConnection, after routing to target_app
if (target_app) |a| {
    // Check if app is draining
    if (self.app_drain_state.get(hostname)) |drain| {
        if (drain.draining) {
            // Return 503 immediately
            try self.sendResponse(conn, 503, "application/json",
                "{\"error\":\"App draining\"}");
            return;
        }
        // Increment active connections
        if (self.app_drain_state.getPtr(hostname)) |drain_mut| {
            drain_mut.active_connections += 1;
        }
    }
}

// After request completes, decrement
defer {
    if (target_app and host) |hostname_val| {
        if (self.app_drain_state.getPtr(hostname_val)) |drain| {
            drain.active_connections -|= 1;
        }
    }
}
```

**Key insight:** Check draining state BEFORE incrementing connection count to ensure 503 responses don't falsely increment counters.

### Pattern 3: Drain-Wait Sequence

**What:** After SIGTERM, stop accepting, mark apps as draining, then wait for connections to complete or timeout.

**When to use:** Signal handler calls this sequence.

**Implementation:**

```zig
pub fn initiateGracefulShutdown(self: *HttpServer, timeout_ms: u64) void {
    var logger = log.stdout();

    logger.info("shutdown_initiated", .{
        .reason = "signal",
        .timeout_ms = timeout_ms,
    });

    // Stop accepting new connections
    self.running = false;

    // Mark all apps as draining
    var iter = self.app_drain_state.valueIterator();
    while (iter.next()) |drain| {
        drain.draining = true;
        drain.drain_start_ns = std.time.nanoTimestamp();
    }

    const start_ns = std.time.nanoTimestamp();
    const timeout_ns = timeout_ms * 1_000_000;

    // Poll for drain completion
    var poll_count: u32 = 0;
    while (true) {
        // Check if all connections are closed
        var all_empty = true;
        var iter2 = self.app_drain_state.valueIterator();
        while (iter2.next()) |drain| {
            if (drain.active_connections > 0) {
                all_empty = false;
                break;
            }
        }

        if (all_empty) {
            logger.info("shutdown_drained", .{.polls = poll_count});
            break;
        }

        // Check timeout
        const elapsed_ns = std.time.nanoTimestamp() - start_ns;
        if (elapsed_ns > timeout_ns) {
            logger.warn("shutdown_timeout", .{
                .elapsed_ms = elapsed_ns / 1_000_000,
                .timeout_ms = timeout_ms,
            });
            break;
        }

        // Small sleep to avoid busy-loop (10ms)
        std.time.sleep(10_000_000);
        poll_count += 1;
    }
}
```

**Key insight:** Use nanosecond timestamps for precise timeout measurement. Poll with small sleep to avoid busy-looping while remaining responsive.

### Pattern 4: App Removal with Drain

**What:** When config watcher removes an app, drain that app before deallocating.

**When to use:** In `removeApp()` (http.zig lines 245-280) and config reload handler.

**Implementation:**

```zig
fn removeApp(self: *HttpServer, hostname: []const u8) void {
    var logger = log.stdout();

    // Mark app as draining (don't delete from drain_state yet)
    if (self.app_drain_state.getPtr(hostname)) |drain| {
        drain.draining = true;
        drain.drain_start_ns = std.time.nanoTimestamp();
    }

    logger.info("app_drain_start", .{.hostname = hostname});

    // Wait for connections to drain (with timeout)
    const drain_timeout_ms: u64 = 30_000; // 30s default
    const start_ns = std.time.nanoTimestamp();

    while (true) {
        if (self.app_drain_state.get(hostname)) |drain| {
            if (drain.active_connections == 0) {
                logger.info("app_drained", .{.hostname = hostname});
                break;
            }

            const elapsed_ns = std.time.nanoTimestamp() - start_ns;
            if (elapsed_ns > drain_timeout_ms * 1_000_000) {
                logger.warn("app_drain_timeout", .{
                    .hostname = hostname,
                    .active = drain.active_connections,
                });
                break;
            }
        }

        std.time.sleep(10_000_000); // 10ms poll
    }

    // Now proceed with actual removal (existing code)
    if (self.apps.fetchRemove(hostname)) |kv| {
        // ... existing removal logic ...
    }

    // Clean up drain state
    _ = self.app_drain_state.remove(hostname);
}
```

**Key insight:** Don't block the accept loop during app removal drain. This pattern should be called from the config reload callback on the event loop.

### Anti-Patterns to Avoid

- **Blocking accept() loop during drain:** Would prevent new requests from being rejected with 503. Instead, mark app draining and return 503 in request path.
- **Counting connections by incrementing in handleConnection start and not decrementing on error:** Connection count becomes inaccurate. Always use defer to decrement.
- **Using global timeout for both server and app drains:** Server drain should have one timeout, per-app removal drains could have different timeouts. Make timeouts configurable.
- **Not cleaning up drain state after removal:** `app_drain_state` entries should be removed after app is deallocated to prevent memory leak.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|------------|-------------|-----|
| **Signal handling** | Custom signal dispatch system | std.posix.Sigaction + global server ref | Proven pattern, used in line 912-920, minimal code |
| **Connection counting** | Manual thread-safe atomic counter | u64 with saturation (drain.active_connections -\|= 1) | Single-threaded MVP, saturation prevents underflow |
| **Drain-wait polling** | Custom spin-loop with busy-wait | std.time.sleep(10ms) + nanoTimestamp() timeout | Low-overhead, prevents CPU spinning |
| **Graceful close of listening socket** | Shutdown SHUT_RDWR + close | Dummy connection pattern (line 571) | Already proven in this codebase, reliable |
| **Drain state machine** | Complex state enum with transitions | Simple bool flags (running, draining) | MVP sufficient, can extend if needed |

**Key insight:** Graceful shutdown is not a uniquely hard problem. Existing Zig stdlib and NANO's proven patterns (signal handlers, connection handling) are sufficient. The main addition is connection tracking—a simple counter per app.

## Common Pitfalls

### Pitfall 1: Accepting New Connections After SIGTERM

**What goes wrong:** Signal handler sets `server.running = false`, but accept loop doesn't check this flag before accepting:
```zig
while (self.running) {
    const conn = self.server.accept() catch |err| {
        if (!self.running) break;  // WRONG: checked AFTER accept
        // ...
    };
    // Now accept returned before running was checked!
}
```

**Why it happens:** accept() is blocking; it doesn't return until a connection is ready. After it returns, running flag might have changed.

**How to avoid:**
- Check `running` flag IMMEDIATELY after accept(), before any work (line 379 in current code does this)
- Use the dummy connection wake pattern (line 571) to unblock accept() when shutting down
- Test: Send SIGTERM while accept is blocked, verify dummy connection unblocks it

**Warning signs:**
- Seeing "503 Service Unavailable" responses after signal handler runs
- Process taking >30s to exit (drain timeout firing) on SIGTERM

### Pitfall 2: Connection Count Underflow

**What goes wrong:** Decrementing connection counter can go negative if increments and decrements race:
```zig
if (some_condition) {
    drain.active_connections += 1;
}
// ... later ...
drain.active_connections -= 1;  // If condition was false, now underflow!
```

**Why it happens:** Connection count increment is conditional (only if target_app exists), but decrement is unconditional.

**How to avoid:**
- Use saturating subtract: `drain.active_connections -|= 1`
- Increment and decrement in symmetric pairs (both conditional or both unconditional)
- Add assertion before decrement: `assert(drain.active_connections > 0)`
- Test: Create invalid request (no matching app), verify counter doesn't underflow

**Warning signs:**
- Drain loop never exits (counter stuck at large number due to underflow)
- Wraparound visible in logs (99...99 active connections)

### Pitfall 3: Drain Timeout Too Short or Missing

**What goes wrong:** App removal happens, but drain timeout is too short, process exits while requests are still pending:
```zig
// Hardcoded 1s timeout—not enough for slow apps
std.time.sleep(1_000_000_000);
```

**Why it happens:** Timeout chosen arbitrarily without testing against real request latencies.

**How to avoid:**
- Default timeout: 30s (long enough for slow requests, short enough to not hang forever)
- Make timeout configurable in config.json per app
- Test: Create slow fetch() request (5s), trigger app removal, verify it completes before drain
- Log actual drain duration for performance tuning

**Warning signs:**
- Requests returning 503 when they shouldn't have completed yet
- Process exiting too quickly on SIGTERM (< 5s on high-traffic server)

### Pitfall 4: Memory Leak in app_drain_state

**What goes wrong:** App removed and deallocated, but entry in `app_drain_state` map remains:
```zig
fn removeApp(self: *HttpServer, hostname: []const u8) void {
    // ... drain app ...
    // Remove from apps map
    _ = self.apps.fetchRemove(hostname);
    // BUG: forgot to remove from app_drain_state
}
```

**Why it happens:** Easy to forget to clean up the drain state map after app removal.

**How to avoid:**
- Add explicit cleanup step: `_ = self.app_drain_state.remove(hostname);` after app removal
- Consider using a paired HashMap removal helper function to ensure both are cleaned
- Test: Load app, remove via config, reload config, check memory doesn't grow unbounded

**Warning signs:**
- Memory usage growing with each app removal
- app_drain_state map size larger than active app count

### Pitfall 5: Drain Blocks Event Loop

**What goes wrong:** Drain-wait loop in removeApp() blocks the event loop, preventing new config reloads or timers from firing:
```zig
fn removeApp(self: *HttpServer, hostname: []const u8) void {
    // ... BLOCKING while loop checking active_connections ...
    // config_watcher callback is blocked!
}
```

**Why it happens:** Tempting to drain synchronously in removeApp(), but that blocks the event loop.

**How to avoid:**
- Move app drain into async or fire-and-forget cleanup task
- OR: Don't wait for drain in removeApp(), just mark as draining and let request path return 503
- Per-app drains should not block server from accepting new requests
- Test: Remove two apps rapidly in config reload, verify both drain in parallel

**Warning signs:**
- Config reload hangs while waiting for app to drain
- New requests rejected even though server is running
- Event loop timers not firing during app removal

### Pitfall 6: Race Between Check and Increment

**What goes wrong:** Thread checks app is not draining, but app drains before increment:
```
Thread A: if (!drain.draining)
Thread B: drain.draining = true
Thread A: drain.active_connections += 1  // Incremented draining app!
```

**Why it happens:** Single-threaded MVP means this shouldn't happen, but it's still a risk if code becomes threaded.

**How to avoid:**
- Single-threaded MVP is safe from thread races
- If threading is added in future, use atomic operations or locks
- For now, comment code: `// Note: Single-threaded, no thread-safety needed`
- Add test: Rapid signal + request, verify 503 is returned correctly

**Warning signs:**
- Requests completing on apps that were marked draining
- Active connection count not decrementing after drain timeout

## Code Examples

Verified patterns from standard sources and existing NANO code:

### Example 1: Signal Handler (from http.zig lines 881-885)

```zig
// Source: Existing NANO signal handler pattern (http.zig)
// Usage: Install SIGTERM/SIGINT handlers for graceful shutdown

fn handleSignal(_: c_int) callconv(.c) void {
    if (global_server) |s| {
        s.stop();  // Will call initiateGracefulShutdown()
    }
}

// In serveMultiApp():
const sigterm_action = posix.Sigaction{
    .handler = .{ .handler = handleSignal },
    .mask = std.posix.sigemptyset(),
    .flags = 0,
};
_ = posix.sigaction(posix.SIG.TERM, &sigterm_action, null);
_ = posix.sigaction(posix.SIG.INT, &sigterm_action, null);
```

### Example 2: Connection Tracking Initialization

```zig
// Source: NANO pattern from http.zig (HttpServer.init)
// Usage: Initialize per-app connection tracking on server start

pub fn init(port: u16, allocator: std.mem.Allocator) !HttpServer {
    // ... existing init code ...

    return HttpServer{
        // ... existing fields ...
        .app_drain_state = std.StringHashMap(AppDrainState).init(allocator),
    };
}
```

### Example 3: Increment Connection Count (in handleConnection)

```zig
// Source: NANO request handling pattern (http.zig handleConnection)
// Usage: Track active connections for accurate drain completion detection

var active_connection_tracked = false;
defer {
    if (active_connection_tracked) {
        if (host) |hostname| {
            if (self.app_drain_state.getPtr(hostname)) |drain| {
                drain.active_connections -|= 1;
            }
        }
    }
}

// After routing to target_app, check if draining
if (target_app) |_| {
    if (host) |hostname| {
        if (self.app_drain_state.get(hostname)) |drain| {
            if (drain.draining) {
                try self.sendResponse(conn, 503, "application/json",
                    "{\"error\":\"Service draining\"}");
                return;
            }
            // Increment and mark tracked
            if (self.app_drain_state.getPtr(hostname)) |drain_mut| {
                drain_mut.active_connections += 1;
                active_connection_tracked = true;
            }
        }
    }
}
```

### Example 4: Drain-Wait Loop on SIGTERM

```zig
// Source: NANO graceful shutdown pattern (new)
// Usage: Block process exit until connections drain or timeout

pub fn stop(self: *HttpServer) void {
    self.running = false;

    var logger = log.stdout();

    // Unblock accept() with dummy connection
    const wake_conn = std.net.tcpConnectToAddress(self.address) catch null;
    if (wake_conn) |conn| {
        conn.close();
    }

    logger.info("server_stop", .{
        .requests = self.metrics.request_count,
        .errors = self.metrics.error_count,
    });

    // NEW: Initiate graceful drain
    self.initiateGracefulShutdown(30_000);  // 30s timeout
}

fn initiateGracefulShutdown(self: *HttpServer, timeout_ms: u64) void {
    var logger = log.stdout();

    logger.info("shutdown_graceful", .{.timeout_ms = timeout_ms});

    // Mark all apps as draining
    var iter = self.app_drain_state.valueIterator();
    while (iter.next()) |drain| {
        drain.draining = true;
        drain.drain_start_ns = std.time.nanoTimestamp();
    }

    const start_ns = std.time.nanoTimestamp();
    const timeout_ns = timeout_ms * 1_000_000;

    // Poll for drain completion
    while (true) {
        var all_empty = true;
        var iter2 = self.app_drain_state.valueIterator();
        while (iter2.next()) |drain| {
            if (drain.active_connections > 0) {
                all_empty = false;
                break;
            }
        }

        if (all_empty) {
            logger.info("shutdown_drained", .{});
            break;
        }

        const elapsed_ns = std.time.nanoTimestamp() - start_ns;
        if (elapsed_ns > timeout_ns) {
            logger.warn("shutdown_timeout_reached", .{});
            break;
        }

        std.time.sleep(10_000_000);  // 10ms poll
    }
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Hard kill on signal | Graceful drain + timeout | Kubernetes / systemd adoption (2010s+) | Prevents data loss, better UX, required in prod |
| Process tracks all connections globally | Per-app connection tracking | Multi-tenant / microservices era (2015+) | Accurate drain detection, faster removal |
| Infinite drain wait | Drain with timeout | Operational maturity (2018+) | Prevents zombie processes, predictable shutdown |
| No app removal draining | Drain before removal | Config-hot-reload systems (2020+) | Prevents request failures on config change |
| 503 hardcoded on errors | 503 for intentional draining | Load balancer integration (2020+) | Enables graceful rebalancing |

**Deprecated/outdated:**
- Killing process on signal without draining: causes data loss, rejected by production systems
- Buffering entire response during drain: streaming (v1.2-03) makes this obsolete

## Open Questions

1. **Drain timeout configurability**
   - What we know: 30s default is reasonable for most web apps
   - What's unclear: Should timeout be global, per-app, or both?
   - Recommendation: Global 30s default for process shutdown, per-app 30s for config removal. Make both configurable in config.json under `shutdown` section.

2. **Draining with new config reload**
   - What we know: Config reload calls removeApp() which drains
   - What's unclear: If two apps are removed in same reload, do they drain sequentially or in parallel?
   - Recommendation: Drain in parallel (mark both as draining, poll both, don't block). Requires non-blocking removeApp().

3. **503 response headers and body**
   - What we know: WHATWG requires Content-Length, HTTP spec allows any body
   - What's unclear: Should 503 include Retry-After header? Should body indicate reason?
   - Recommendation: Include `Retry-After: 30` header (drain timeout value) and JSON body `{"error":"Service draining","retry_after_s":30}`. Allows smart clients to back off.

4. **Metrics during shutdown**
   - What we know: metrics are recorded in HttpServer
   - What's unclear: Should drain poll iterations, app drain times, signal delays be recorded?
   - Recommendation: Log shutdown events at INFO level (start, drained, timeout, exit). Add optional metrics fields for performance analysis.

5. **Integration with event loop cleanup**
   - What we know: EventLoop has timers that need cleanup on shutdown
   - What's unclear: Should timers be cancelled during drain, or just let them fire?
   - Recommendation: Cancel active timers during drain-wait (prevents spurious callbacks after exit). Requires EventLoop.cancelAll() or similar.

## Sources

### Primary (HIGH confidence)
- **NANO current http.zig signal handler** (lines 878-920) - Existing pattern for signal setup
- **NANO http.zig connection handling** (lines 353-389, 391-542) - Existing request routing and logging
- **NANO config watcher and app removal** (http.zig lines 137-280) - Hot-reload pattern to extend
- **Zig std.posix signal handling** - Standard library capability

### Secondary (MEDIUM confidence)
- **Envoy Proxy Graceful Shutdown** (https://gateway.envoyproxy.io/docs/tasks/operations/graceful-shutdown/) - Production implementation reference
- **Go Graceful Shutdown Patterns** (https://victoriametrics.com/blog/go-graceful-shutdown/) - Language-agnostic patterns
- **HTTP 503 Service Unavailable** (https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/503) - Status code semantics
- **Node.js HTTP Keep-Alive Connection Draining** (https://github.com/nodejs/node/issues/60617) - Similar problem domain

### Tertiary (validation needed)
- **libxev timer cancellation** (https://github.com/mitchellh/libxev) - Event loop integration for cleanup (spike recommended)

### Code References
- /Users/gleicon/code/zig/nano/src/server/http.zig - HttpServer struct, signal handlers, connection handling
- /Users/gleicon/code/zig/nano/src/runtime/event_loop.zig - EventLoop and ConfigWatcher patterns
- /Users/gleicon/code/zig/nano/src/log.zig - Logging pattern for shutdown events

## Metadata

**Confidence breakdown:**
- Signal handling: HIGH - Already implemented in codebase, pattern proven
- Connection tracking: HIGH - Simple counter, no concurrency in single-threaded MVP
- Drain timeout: HIGH - Standard pattern across Go, Rust, Node.js servers
- App removal drain: MEDIUM-HIGH - Depends on safe non-blocking integration with config watcher
- Event loop cleanup: MEDIUM - Requires spike on EventLoop timer cancellation

**Research date:** 2026-02-07
**Valid until:** 2026-02-21 (stable patterns, implementation may surface edge cases)

**Phase Dependencies:**
- **Blocks:** v1.2-05 (Documentation should include shutdown section)
- **Depends on:** v1.2-01, v1.2-02, v1.2-03 (All completed, graceful shutdown is independent feature)
- **Independent from:** Streams and env vars features

## Next Steps for Planner

1. **Spike: Event loop timer cleanup on shutdown** - Verify EventLoop has method to cancel all pending timers, or implement if needed
2. **Architecture review:** Decide on AppDrainState struct location (http.zig or separate module)
3. **Test matrix planning:**
   - Single app removal (app drains, process continues)
   - SIGTERM with slow request (request completes, then exit)
   - SIGTERM with no requests (immediate drain)
   - Multiple app removals (parallel draining)
   - Drain timeout trigger (process exits after 30s)
4. **Configuration decision:** Should drain timeout be configurable in config.json? Recommend: yes, `"shutdown": {"drain_timeout_ms": 30000}`
5. **Metrics/logging:** Define what shutdown events should be logged (recommend: start, per-app drain start/complete, process exit)
6. **503 response format:** Finalize whether to include Retry-After header and what JSON body looks like

## Next Steps for Phase Executor

Once planning is complete:
1. Create 3 tasks: Connection Tracking → Draining State Machine → Drain-Wait Sequence
2. Start with connection tracking (simple, no dependencies)
3. Implement draining state checks in request routing (reuses existing handleConnection path)
4. Implement drain-wait in signal handler
5. Extend removeApp() to drain per-app connections
6. Integration test: SIGTERM with active request, verify completes before exit

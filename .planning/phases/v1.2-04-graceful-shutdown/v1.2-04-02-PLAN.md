---
phase: v1.2-04-graceful-shutdown
plan: 02
type: execute
wave: 2
depends_on:
  - v1.2-04-01
files_modified:
  - src/server/http.zig
autonomous: true

must_haves:
  truths:
    - "SIGTERM/SIGINT stops accepting new connections and drains in-flight requests"
    - "New requests to a draining app receive 503 Service Unavailable"
    - "Removing an app via config watcher waits for its connections to drain before deallocation"
    - "Process exits after drain timeout (30s default) even if connections hang"
    - "In-flight requests complete before process exits"
  artifacts:
    - path: "src/server/http.zig"
      provides: "Full graceful shutdown: 503 routing, connection counting, drain-wait sequence"
      contains: "initiateGracefulShutdown"
  key_links:
    - from: "handleConnection"
      to: "app_drain_state draining check"
      via: "503 response before incrementing counter"
      pattern: "drain\\.draining"
    - from: "stop()"
      to: "initiateGracefulShutdown"
      via: "called after setting running=false"
      pattern: "initiateGracefulShutdown"
    - from: "removeApp"
      to: "drain poll loop"
      via: "marks draining, waits for active_connections==0"
      pattern: "active_connections == 0"
---

<objective>
Implement the three drain behaviors: (1) connection counting with 503 for draining apps in handleConnection, (2) per-app drain wait in removeApp before deallocation, (3) process-wide drain wait in stop() via initiateGracefulShutdown. Together these satisfy all six SHUT requirements.

Purpose: Operators need confidence that removing an app or killing the process does not drop in-flight requests. This plan makes NANO production-safe for zero-downtime deployments and clean shutdowns.
Output: Full graceful shutdown behavior — 503 for draining apps, connection counting, drain loops with 30s timeout.
</objective>

<execution_context>
@/Users/gleicon/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gleicon/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/server/http.zig
@.planning/phases/v1.2-04-graceful-shutdown/v1.2-04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add connection counting and 503 drain check to handleConnection</name>
  <files>src/server/http.zig</files>
  <action>
Modify `handleConnection` to track active connections per app and return 503 when the app is draining.

Locate the section in `handleConnection` after target_app is resolved (after the `if (target_app == null) { target_app = self.default_app; }` block, approximately line 477). Insert BEFORE the `var status: u16 = 200;` line:

```zig
        // Graceful shutdown: check if target app is draining, count active connections
        // Note: Single-threaded, no thread-safety needed for counter operations
        var active_connection_tracked = false;
        defer {
            if (active_connection_tracked) {
                if (host) |hostname| {
                    if (self.app_drain_state.getPtr(hostname)) |drain| {
                        drain.active_connections -|= 1; // Saturating subtract prevents underflow
                    }
                }
            }
        }

        if (target_app != null) {
            if (host) |hostname| {
                if (self.app_drain_state.get(hostname)) |drain| {
                    if (drain.draining) {
                        // App is being removed or server is shutting down
                        try self.sendResponse(conn, 503, "application/json",
                            "{\"error\":\"Service draining\",\"retry_after_s\":30}");
                        const latency_ns: u64 = @intCast(std.time.nanoTimestamp() - start_time);
                        self.metrics.recordRequest(latency_ns, true);
                        logRequest(request_id, method, path, 503, 43, @as(f64, @floatFromInt(latency_ns)) / 1_000_000.0);
                        return;
                    }
                    // Not draining — increment active connection count
                    if (self.app_drain_state.getPtr(hostname)) |drain_mut| {
                        drain_mut.active_connections += 1;
                        active_connection_tracked = true;
                    }
                }
            }
        }
```

Important design notes:
- Check draining BEFORE incrementing to ensure 503 responses don't falsely inflate counters
- Use saturating subtract (`-|=`) in defer to prevent underflow if code path was unexpected
- Track with boolean flag so defer only decrements if increment actually happened
- 503 body length 43 = len(`{"error":"Service draining","retry_after_s":30}`)
- Single-app mode (self.app path) does not have drain state entries — only multi-app via hostname routing is tracked. This is intentional: single-app mode does not support hot removal.
  </action>
  <verify>
Run `zig build` — must compile. Start server with multi-app config, send a request to a loaded app, confirm it still returns 200 (draining=false by default). Then manually test by temporarily hard-coding `draining: true` in AppDrainState{} init for one app and verifying 503 response.
  </verify>
  <done>handleConnection returns 503 for draining apps and correctly increments/decrements active_connections counter for non-draining apps. Normal request flow unchanged.</done>
</task>

<task type="auto">
  <name>Task 2: Implement drain-wait in removeApp and initiateGracefulShutdown in stop</name>
  <files>src/server/http.zig</files>
  <action>
Two coordinated changes:

**A. Modify removeApp() to drain before deallocation:**

In `removeApp()`, BEFORE the existing `if (self.apps.fetchRemove(hostname)) |kv|` block, insert:

```zig
        // Mark app as draining before removal to prevent new connections
        if (self.app_drain_state.getPtr(hostname)) |drain| {
            drain.draining = true;
            drain.drain_start_ns = std.time.nanoTimestamp();
        }

        logger.info("app_drain_start", .{.hostname = hostname});

        // Wait for active connections to complete (with timeout)
        const drain_timeout_ns: i128 = 30_000 * 1_000_000; // 30s in nanoseconds
        const drain_start = std.time.nanoTimestamp();

        while (true) {
            if (self.app_drain_state.get(hostname)) |drain| {
                if (drain.active_connections == 0) {
                    logger.info("app_drained", .{.hostname = hostname});
                    break;
                }
                const elapsed = std.time.nanoTimestamp() - drain_start;
                if (elapsed > drain_timeout_ns) {
                    logger.warn("app_drain_timeout", .{
                        .hostname = hostname,
                        .active = drain.active_connections,
                    });
                    break;
                }
            } else {
                break; // No drain state = no connections tracked
            }
            std.time.sleep(10_000_000); // 10ms poll
        }
```

This goes BEFORE the `if (self.apps.fetchRemove(hostname)) |kv|` block so the app stays in the maps during drain (returning 503) and is only deallocated after draining.

Also ensure `_ = self.app_drain_state.remove(hostname);` inside the fetchRemove block happens BEFORE `self.allocator.free(kv.key)`. Current code frees kv.key at the end — move the drain_state removal to just after finding the app in storage but before freeing kv.key. Specifically, inside the `if (self.apps.fetchRemove(hostname)) |kv|` block, add after `self.app_storage.swapRemove(i)` and before the `break`:

```zig
                    // Clean up drain state (uses hostname pointer which is still valid)
                    _ = self.app_drain_state.remove(hostname);
```

Wait — note that at this point `kv.key` and `hostname` point to the same memory (the duped key). We use `hostname` (the parameter, pointing to same allocation) for the drain state removal. This is safe because we haven't freed `kv.key` yet at that point in the function. Verify the free order in the existing code.

**B. Add initiateGracefulShutdown() method and call it from stop():**

Add a new private method to HttpServer after the `stop()` method:

```zig
    /// Wait for all active connections to complete or timeout
    fn initiateGracefulShutdown(self: *HttpServer, timeout_ms: u64) void {
        var logger = log.stdout();

        // Mark all apps as draining
        var iter = self.app_drain_state.valueIterator();
        while (iter.next()) |drain| {
            drain.draining = true;
            drain.drain_start_ns = std.time.nanoTimestamp();
        }

        logger.info("shutdown_graceful", .{.timeout_ms = timeout_ms});

        const start_ns = std.time.nanoTimestamp();
        const timeout_ns: i128 = @intCast(timeout_ms * 1_000_000);
        var poll_count: u32 = 0;

        // Poll until all connections complete or timeout
        while (true) {
            var all_drained = true;
            var check_iter = self.app_drain_state.valueIterator();
            while (check_iter.next()) |drain| {
                if (drain.active_connections > 0) {
                    all_drained = false;
                    break;
                }
            }

            if (all_drained) {
                logger.info("shutdown_drained", .{.polls = poll_count});
                break;
            }

            const elapsed_ns = std.time.nanoTimestamp() - start_ns;
            if (elapsed_ns > timeout_ns) {
                logger.warn("shutdown_timeout_reached", .{
                    .elapsed_ms = @divTrunc(elapsed_ns, 1_000_000),
                    .timeout_ms = timeout_ms,
                });
                break;
            }

            std.time.sleep(10_000_000); // 10ms poll
            poll_count += 1;
        }
    }
```

Modify `stop()` to call initiateGracefulShutdown AFTER the existing dummy connection wake pattern:

```zig
    pub fn stop(self: *HttpServer) void {
        self.running = false;

        // Unblock accept() by making a dummy connection to ourselves
        const wake_conn = std.net.tcpConnectToAddress(self.address) catch null;
        if (wake_conn) |conn| {
            conn.close();
        }

        var logger = log.stdout();
        logger.info("server_stop", .{
            .requests = self.metrics.request_count,
            .errors = self.metrics.error_count,
            .uptime_s = self.metrics.uptimeSeconds(),
        });

        // Drain active connections before exit (30s timeout)
        self.initiateGracefulShutdown(30_000);
    }
```
  </action>
  <verify>
1. `zig build` succeeds.
2. Start server: `./zig-out/bin/nano serve --config test/config.json`
3. Test SIGTERM drain: Start a slow request (if no slow test app exists, just verify server exits cleanly on Ctrl+C with "shutdown_drained" in logs)
4. Test app removal drain: Modify config to remove an app, verify logs show "app_drain_start" then "app_drained" then "app_removed"
5. Confirm normal requests still work: `curl -H "Host: localhost" http://127.0.0.1:3000/` returns expected response
  </verify>
  <done>
- SIGTERM triggers graceful drain sequence: logs show shutdown_graceful, then shutdown_drained (or shutdown_timeout_reached after 30s)
- App removal via config watcher shows app_drain_start, app_drained, app_removed in sequence
- 503 returned for requests to draining app (manually verifiable by triggering removal during request)
- Server exits cleanly after drain
  </done>
</task>

</tasks>

<verification>
1. `zig build` succeeds with no errors
2. `rg "initiateGracefulShutdown" src/server/http.zig` shows definition and call in stop()
3. `rg "503" src/server/http.zig` shows the drain 503 response in handleConnection
4. Server starts and handles requests normally (no regression)
5. Ctrl+C (SIGINT) shows shutdown logs and clean exit
6. Config reload removing an app shows drain sequence in logs
</verification>

<success_criteria>
- SHUT-01: SIGTERM/SIGINT triggers graceful shutdown sequence via existing handleSignal -> stop() -> initiateGracefulShutdown()
- SHUT-02: stop() sets running=false before draining, accept() loop exits
- SHUT-03: initiateGracefulShutdown polls active_connections with 30s timeout
- SHUT-04: removeApp marks app draining, polls for completion before fetchRemove
- SHUT-05: handleConnection returns 503 when drain.draining is true
- SHUT-06: active_connections counter incremented on connection start, decremented via defer
</success_criteria>

<output>
After completion, create `.planning/phases/v1.2-04-graceful-shutdown/v1.2-04-02-SUMMARY.md`
</output>
